# -*- coding: utf-8 -*-
"""PHR.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EPeMVOdneltcw-EZpjOCxPuRMaOmgve5
"""

# ============================================================
# 1. IMPORTS & BASIC SETUP
# ============================================================
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (
    classification_report,
    confusion_matrix,
    roc_auc_score,
    roc_curve,
    accuracy_score,
    precision_score,
    recall_score,
    f1_score
)

import warnings
warnings.filterwarnings("ignore")

# For pretty plots
plt.style.use("seaborn-v0_8")
sns.set(font_scale=1.1)

# ============================================================
# 2. LOAD DATA
# ============================================================
from google.colab import files
uploaded = files.upload()

"""### DATASET DESCRIPTION

**Data Preview**
"""

import pandas as pd
df = pd.read_csv('/content/blood.csv')
df.head()

"""**Data Types**:"""

# ============================================================
# 3. BASIC DATA EXPLORATION
# ============================================================
df.info()

"""**Summary Statistics:**"""

df.describe().T

"""The summary statistics reveal that:

Donors are generally recent (median Recency ≈ 7 months).

Donation behaviour varies widely, with Frequency ranging from 1 to 50 and Monetary from 250 to 12,500 ml.

Time since first donation also shows substantial variation, suggesting both new and long-term donors.

The Class mean (≈ 0.24) confirms that the dataset is imbalanced, with far fewer returning donors.

Overall, the dataset is clean, well-structured, and suitable for machine learning analysis aimed at predicting future donation behaviour.
"""

# Check for missing values
df.isna().sum()

# Distribution of target variable
df["Class"].value_counts(), df["Class"].value_counts(normalize=True)

# Bar plot of class distribution
plt.figure(figsize=(5,4))
sns.countplot(x="Class", data=df)
plt.title("Class Distribution (Donation vs No Donation)")
plt.xlabel("Class (1 = Donated, 0 = Not Donated)")
plt.ylabel("Count")
plt.show()

"""EXPLANATION: The class distribution is clearly imbalanced. A large majority of donors belong to Class 0 (did not donate again), while only a smaller portion are Class 1 (donated again). This imbalance means accuracy alone will not be a reliable evaluation metric, so recall and F1-score for Class 1 will be more important.

## EDA [EXPLORATORY DATA ANALYSIS] AND CORRELATION
"""

# ============================================================
# 4. CORRELATION & EDA
# ============================================================
corr = df.corr(numeric_only=True)

plt.figure(figsize=(7,5))
sns.heatmap(corr, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Correlation Heatmap - Blood Donation Dataset")
plt.show()

"""EXPLANATION:


*  Frequency and Monetary are almost perfectly correlated because
total blood donated increases directly with the number of donations.
*   Recency has a moderate negative correlation with Class, indicating that recent donors are more likely to return.

*   Frequency and Monetary show positive correlations with Class, suggesting that consistent donors are more likely to donate again.
*   Time also correlates with Frequency and Monetary, but its link with Class is weak.

Overall, Recency and Frequency appear to be the strongest behavioural predictors
"""

# Histograms of features
feature_cols = ["Recency", "Frequency", "Monetary", "Time"]

df[feature_cols].hist(bins=15, figsize=(10,6))
plt.suptitle("Feature Distributions", y=1.02)
plt.tight_layout()
plt.show()

"""EXPLANATION:

*   Recency is heavily concentrated at low values, meaning many donors donated recently.
*   Frequency shows a strong right skew, with most donors giving only a few times and only a handful being very frequent donors.
*   Monetary mirrors Frequency since the amount donated depends on how many times a donor gave blood.
*   Time spans a wide range, indicating a mix of new donors and long-term donors.
"""

# Pairplot coloured by Class (optional: can be slow)
sns.pairplot(df, vars=feature_cols, hue="Class", diag_kind="hist")
plt.suptitle("Pairplot of Features by Class", y=1.02)
plt.show()

"""EXPLANATION:
The pairplot shows that Class 1 points (returning donors) tend to cluster at:

*   lower Recency values
*   higher Frequency and Monetary values
This visually confirms behavioural predictors of donor return probability.
"""

# ============================================================
# 5. TRAIN–TEST SPLIT
# ============================================================
X = df.drop("Class", axis=1)
y = df["Class"]

X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,
    random_state=42,
    stratify=y
)

print("Train shape:", X_train.shape)
print("Test shape:", X_test.shape)

# ============================================================
# 6.1 MODEL 1 – LOGISTIC REGRESSION
# ============================================================
log_reg_pipeline = Pipeline([
    ("scaler", StandardScaler()),
    ("clf", LogisticRegression(max_iter=1000))
])

log_reg_pipeline.fit(X_train, y_train)

y_pred_lr = log_reg_pipeline.predict(X_test)
y_proba_lr = log_reg_pipeline.predict_proba(X_test)[:, 1]

print("Logistic Regression – Classification Report")
print(classification_report(y_test, y_pred_lr))

print("Confusion Matrix (Logistic Regression)")
print(confusion_matrix(y_test, y_pred_lr))

metrics_lr = {
    "accuracy": accuracy_score(y_test, y_pred_lr),
    "precision": precision_score(y_test, y_pred_lr),
    "recall": recall_score(y_test, y_pred_lr),
    "f1": f1_score(y_test, y_pred_lr),
    "roc_auc": roc_auc_score(y_test, y_proba_lr),
}
metrics_lr

# ROC curve – Logistic Regression
fpr_lr, tpr_lr, _ = roc_curve(y_test, y_proba_lr)

plt.figure(figsize=(6,5))
plt.plot(fpr_lr, tpr_lr, label=f"LogReg (AUC = {metrics_lr['roc_auc']:.3f})")
plt.plot([0,1], [0,1], "k--")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve – Logistic Regression")
plt.legend()
plt.show()

# ============================================================
# 6.2. MODEL 2 – RANDOM FOREST
# ============================================================
rf_pipeline = Pipeline([
    ("scaler", StandardScaler()),
    ("clf", RandomForestClassifier(
        n_estimators=300,
        random_state=42,
        class_weight="balanced"
    ))
])

rf_pipeline.fit(X_train, y_train)

y_pred_rf = rf_pipeline.predict(X_test)
y_proba_rf = rf_pipeline.predict_proba(X_test)[:, 1]

print("Random Forest – Classification Report")
print(classification_report(y_test, y_pred_rf))

print("Confusion Matrix (Random Forest)")
print(confusion_matrix(y_test, y_pred_rf))

metrics_rf = {
    "accuracy": accuracy_score(y_test, y_pred_rf),
    "precision": precision_score(y_test, y_pred_rf),
    "recall": recall_score(y_test, y_pred_rf),
    "f1": f1_score(y_test, y_pred_rf),
    "roc_auc": roc_auc_score(y_test, y_proba_rf),
}
metrics_rf

# ROC curve – Random Forest
fpr_rf, tpr_rf, _ = roc_curve(y_test, y_proba_rf)

plt.figure(figsize=(6,5))
plt.plot(fpr_rf, tpr_rf, label=f"Random Forest (AUC = {metrics_rf['roc_auc']:.3f})")
plt.plot([0,1], [0,1], "k--")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve – Random Forest")
plt.legend()
plt.show()

# ============================================================
# 6.3. MODEL COMPARISON SUMMARY
# ============================================================
print("Logistic Regression metrics:", metrics_lr)
print("Random Forest metrics      :", metrics_rf)

# ============================================================
# 7. GRADIENT BOOSTING
# ============================================================
from sklearn.ensemble import GradientBoostingClassifier
gb_model = GradientBoostingClassifier(
    n_estimators=200,
    learning_rate=0.05,
    max_depth=3,
    random_state=42
)

gb_model.fit(X_train, y_train)

y_pred_gb = gb_model.predict(X_test)
y_proba_gb = gb_model.predict_proba(X_test)[:, 1]

from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score

print("Gradient Boosting Classification Report")
print(classification_report(y_test, y_pred_gb))

print("Confusion Matrix")
print(confusion_matrix(y_test, y_pred_gb))

roc_auc_gb = roc_auc_score(y_test, y_proba_gb)
print("ROC-AUC:", roc_auc_gb)

from sklearn.metrics import roc_curve

fpr_gb, tpr_gb, _ = roc_curve(y_test, y_proba_gb)

plt.figure(figsize=(6,5))
plt.plot(fpr_gb, tpr_gb, label=f"Gradient Boosting (AUC = {roc_auc_gb:.3f})")
plt.plot([0,1], [0,1], "k--")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve – Gradient Boosting")
plt.legend()
plt.show()

# ============================================================
# 8. FEATURE IMPORTANCE (from Random Forest)
# ============================================================
rf_model = rf_pipeline.named_steps["clf"]
importances = rf_model.feature_importances_
feature_importance = pd.DataFrame({
    "feature": X.columns,
    "importance": importances
}).sort_values(by="importance", ascending=False)

feature_importance

plt.figure(figsize=(6,4))
sns.barplot(x="importance", y="feature", data=feature_importance)
plt.title("Feature Importance (Random Forest)")
plt.xlabel("Importance")
plt.ylabel("Feature")
plt.tight_layout()
plt.show()

# ============================================================
# 9. PERSONALISED RECOMMENDATION FUNCTION
# ============================================================
# We'll use the better model for recall/F1 – typically Random Forest here.
best_model = rf_pipeline

def generate_recommendation(recency, frequency, monetary, time):
    """
    Input: values for a single individual
    Output: model prediction + human-readable recommendation
    """
    input_df = pd.DataFrame({
        "Recency": [recency],
        "Frequency": [frequency],
        "Monetary": [monetary],
        "Time": [time]
    })

    proba = best_model.predict_proba(input_df)[0, 1]
    pred = 1 if proba >= 0.5 else 0

    if pred == 1:
        recommendation = (
            "The model predicts a HIGH probability that this person will donate "
            "blood again. Encourage regular donation and provide reminders."
        )
    else:
        recommendation = (
            "The model predicts a LOW probability of future donation. "
            "Consider targeted engagement, health education, or personalised "
            "follow-up to motivate donation."
        )

    return {
        "predicted_class": int(pred),
        "probability_of_donation": float(proba),
        "recommendation": recommendation
    }

# Example usage
example = generate_recommendation(recency=2, frequency=20, monetary=5000, time=45)
example

# ============================================================
# 10. OPTIONAL: CROSS-VALIDATION (for report)
# ============================================================
cv_scores_rf = cross_val_score(
    rf_pipeline, X, y,
    cv=5,
    scoring="f1"
)
print("Random Forest 5-fold F1 scores:", cv_scores_rf)
print("Mean F1:", cv_scores_rf.mean())